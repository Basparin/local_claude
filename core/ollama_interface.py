"""
Interfaz de comunicaci√≥n con Ollama
"""

import subprocess
import json
import sys
import platform
import time
from typing import List, Dict, Any, Optional
from monitoring.metrics import get_metrics_collector

class OllamaInterface:
    """Interfaz para comunicarse con Ollama"""
    
    def __init__(self, settings):
        self.settings = settings
        self.current_model = settings.models['current']
        self.is_windows = platform.system() == 'Windows'
        self.ollama_cmd = self._get_ollama_command()
        self.metrics = get_metrics_collector()
    
    def _get_ollama_command(self):
        """Obtener comando ollama apropiado para el sistema"""
        if self.is_windows:
            # En Windows, intentar usar WSL si est√° disponible
            try:
                subprocess.run(['wsl', '--version'], capture_output=True, timeout=5)
                return ['wsl', 'ollama']
            except:
                # Si no hay WSL, usar ollama directo (asumiendo que est√° instalado en Windows)
                return ['ollama']
        else:
            return ['ollama']
    
    def test_connection(self) -> bool:
        """Probar conexi√≥n con Ollama"""
        try:
            # Primero probar el comando b√°sico
            result = subprocess.run(
                self.ollama_cmd + ['list'],
                capture_output=True,
                text=True,
                timeout=10
            )
            if result.returncode != 0:
                return False
                
            # Luego verificar el API
            import urllib.request
            api_urls = ['http://localhost:11434/api/tags']
            
            # En Windows con WSL, tambi√©n probar la IP de WSL
            if self.is_windows:
                try:
                    # Obtener IP de WSL
                    wsl_result = subprocess.run(['wsl', 'ip', 'route', 'show'], 
                                              capture_output=True, text=True, timeout=5)
                    # Parsear IP de WSL (esto es una simplificaci√≥n)
                    api_urls.append('http://172.17.0.1:11434/api/tags')  # IP com√∫n de WSL
                except:
                    pass
            
            for url in api_urls:
                try:
                    urllib.request.urlopen(url, timeout=5)
                    return True
                except:
                    continue
                    
            # Si el API no responde, pero ollama list funciona, a√∫n es v√°lido
            return True
                
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):
            return False
    
    def test_model(self, model_name: str) -> bool:
        """Probar si un modelo espec√≠fico funciona"""
        try:
            # Verificar primero que el modelo est√© en la lista
            result = subprocess.run(
                self.ollama_cmd + ['list'],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode != 0:
                return False
                
            # Buscar el modelo en la lista
            if model_name not in result.stdout:
                return False
                
            # Si est√° en la lista, asumimos que funciona
            # (evitamos el test real que es muy lento con deepseek-r1)
            return True
            
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):
            return False
    
    def chat(self, messages: List[Dict[str, str]], model_name: str = None, task_type: str = None) -> Optional[str]:
        """
        Enviar mensajes a Ollama y obtener respuesta
        
        Args:
            messages: Lista de mensajes en formato [{'role': 'user/assistant', 'content': '...'}]
            model_name: Nombre del modelo a usar (opcional)
        
        Returns:
            Respuesta del modelo o None si hay error
        """
        start_time = time.time()
        
        if model_name is None:
            # üß† SMART MODEL SWITCHING
            if task_type:
                from ..config.settings import Settings
                settings = Settings()
                model_name = settings.get_optimal_model(task_type)
            else:
                model_name = self.current_model
        
        try:
            # Preparar el prompt final
            prompt = self._format_messages_for_ollama(messages)
            
            # Ejecutar ollama
            result = subprocess.run(
                self.ollama_cmd + ['run', model_name, prompt],
                capture_output=True,
                text=True,
                timeout=60  # 1 minuto timeout - m√°s r√°pido
            )
            
            # Calcular tiempo de respuesta
            response_time = time.time() - start_time
            
            if result.returncode == 0:
                # Registrar m√©tricas de √©xito
                self.metrics.log_model_usage(model_name, task_type or 'unknown', response_time)
                return result.stdout.strip()
            else:
                # Registrar error
                self.metrics.log_error('ollama_execution', result.stderr, {
                    'model': model_name,
                    'task_type': task_type
                })
                print(f"‚ùå Error de Ollama: {result.stderr}")
                return None
                
        except subprocess.TimeoutExpired:
            response_time = time.time() - start_time
            self.metrics.log_error('ollama_timeout', f"Timeout despu√©s de {response_time:.1f}s", {
                'model': model_name,
                'task_type': task_type
            })
            print("‚ùå Timeout: El modelo tard√≥ demasiado en responder")
            return None
        except FileNotFoundError:
            self.metrics.log_error('ollama_not_found', "Ollama no encontrado", {
                'model': model_name,
                'task_type': task_type
            })
            print("‚ùå Ollama no encontrado. Aseg√∫rate de que est√© instalado y en el PATH")
            return None
        except Exception as e:
            response_time = time.time() - start_time
            self.metrics.log_error('ollama_unexpected', str(e), {
                'model': model_name,
                'task_type': task_type,
                'response_time': response_time
            })
            print(f"‚ùå Error inesperado: {e}")
            return None
    
    def _format_messages_for_ollama(self, messages: List[Dict[str, str]]) -> str:
        """
        Formatear mensajes para Ollama
        
        Ollama no maneja el formato de mensajes estructurados como OpenAI,
        as√≠ que convertimos a un prompt simple
        """
        system_prompt = """Eres Claude, un asistente de IA especializado en programaci√≥n y an√°lisis de c√≥digo.
Est√°s corriendo localmente a trav√©s de Ollama.

Caracter√≠sticas:
- Eres experto en programaci√≥n, an√°lisis de c√≥digo y tareas de desarrollo
- Puedes explorar archivos y directorios
- Ayudas a crear, editar y analizar c√≥digo
- Respondes de manera concisa y pr√°ctica
- Usas emojis apropiados para hacer las respuestas m√°s claras

Contexto actual: Est√°s en una CLI local similar a Claude Code."""
        
        # Construir prompt
        prompt_parts = [system_prompt, "\n\n"]
        
        for message in messages:
            role = message['role']
            content = message['content']
            
            if role == 'user':
                prompt_parts.append(f"Usuario: {content}\n")
            elif role == 'assistant':
                prompt_parts.append(f"Asistente: {content}\n")
        
        prompt_parts.append("Asistente: ")
        
        return "".join(prompt_parts)
    
    def get_available_models(self) -> List[str]:
        """Obtener lista de modelos disponibles"""
        try:
            result = subprocess.run(
                self.ollama_cmd + ['list'],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # Saltar header
                models = []
                for line in lines:
                    if line.strip():
                        model_name = line.split()[0]
                        models.append(model_name)
                return models
            else:
                return []
                
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):
            return []
    
    def switch_model(self, model_name: str) -> bool:
        """Cambiar modelo actual"""
        if self.test_model(model_name):
            self.current_model = model_name
            return True
        return False
    
    def get_model_info(self, model_name: str = None) -> Dict[str, Any]:
        """Obtener informaci√≥n de un modelo"""
        if model_name is None:
            model_name = self.current_model
        
        # Informaci√≥n b√°sica (podr√≠a expandirse)
        return {
            'name': model_name,
            'current': model_name == self.current_model,
            'available': self.test_model(model_name)
        }