# LocalClaude

Una CLI inteligente que replica las capacidades de Claude Code usando Ollama localmente.

## ğŸš€ CaracterÃ­sticas

- **ExploraciÃ³n inteligente** del workspace
- **GestiÃ³n automÃ¡tica de contexto** con compresiÃ³n
- **ComunicaciÃ³n natural** con modelos locales
- **Comandos especializados** para desarrollo
- **Memoria persistente** entre sesiones

## ğŸ“‹ Requisitos

- Python 3.8+
- Ollama instalado y corriendo
- Modelos recomendados:
  - `deepseek-r1:8b` (principal)
  - `qwen2.5-coder:1.5b` (rÃ¡pido)

## ğŸ› ï¸ InstalaciÃ³n

1. Clona/descarga este proyecto
2. Instala Ollama: https://ollama.ai/
3. Descarga los modelos recomendados:
   ```bash
   ollama pull deepseek-r1:8b
   ollama pull qwen2.5-coder:1.5b
   ```

## ğŸ¯ Uso

```bash
cd local_claude
python main.py
```

## ğŸ“š Comandos Disponibles

### ExploraciÃ³n
- `/ls [path]` - Listar archivos
- `/cat <file>` - Mostrar contenido
- `/grep <pattern>` - Buscar en archivos
- `/tree [path]` - Estructura de directorios
- `/find <pattern>` - Buscar archivos

### Sistema
- `/status` - Estado del sistema
- `/context` - InformaciÃ³n del contexto
- `/clear` - Limpiar contexto
- `/model [name]` - Cambiar modelo
- `/help` - Ayuda completa
- `/exit` - Salir

### ConversaciÃ³n Natural
Simplemente escribe cualquier pregunta o solicitud.

## ğŸ—ï¸ Arquitectura

```
local_claude/
â”œâ”€â”€ main.py                    # Punto de entrada
â”œâ”€â”€ core/                      # Motor principal
â”‚   â”œâ”€â”€ cli_engine.py         # Motor de la CLI
â”‚   â”œâ”€â”€ ollama_interface.py   # ComunicaciÃ³n con Ollama
â”‚   â””â”€â”€ command_processor.py  # Procesamiento de comandos
â”œâ”€â”€ context/                   # GestiÃ³n de contexto
â”‚   â””â”€â”€ context_manager.py    # Memoria y compresiÃ³n
â”œâ”€â”€ workspace/                 # ExploraciÃ³n
â”‚   â””â”€â”€ explorer.py           # NavegaciÃ³n inteligente
â”œâ”€â”€ ui/                        # Interfaz de usuario
â”‚   â””â”€â”€ interface.py          # PresentaciÃ³n
â””â”€â”€ config/                    # ConfiguraciÃ³n
    â”œâ”€â”€ settings.py           # ConfiguraciÃ³n general
    â””â”€â”€ models.py             # GestiÃ³n de modelos
```

## ğŸ”§ ConfiguraciÃ³n

El sistema se configura automÃ¡ticamente, pero puedes personalizar:

- **Modelos**: Edita `config/settings.py`
- **Contexto**: Ajusta lÃ­mites de tokens
- **Interfaz**: Habilita/deshabilita colores

## ğŸš¦ Estado del Proyecto

### âœ… Fase 1: Fundamentos (Completada)
- [x] Motor base de CLI
- [x] IntegraciÃ³n con Ollama
- [x] ExploraciÃ³n de workspace
- [x] GestiÃ³n de contexto bÃ¡sica

### â³ Fase 2: Inteligencia (Pendiente)
- [ ] Capacidades de construcciÃ³n
- [ ] AnÃ¡lisis de cÃ³digo avanzado
- [ ] GeneraciÃ³n automÃ¡tica

### â³ Fase 3: Experiencia (Pendiente)
- [ ] Memoria persistente
- [ ] Interfaz pulida
- [ ] Comandos avanzados

## ğŸ¤– Modelos Soportados

- **deepseek-r1:8b**: Razonamiento complejo, anÃ¡lisis profundo
- **qwen2.5-coder:1.5b**: Tareas rÃ¡pidas, cÃ³digo simple
- **qwen2.5:7b**: PropÃ³sito general
- **codellama:7b**: Especializado en cÃ³digo

## ğŸ“Š Performance

- **Tiempo de respuesta**: < 3 segundos (tareas simples)
- **Uso de memoria**: < 2GB total
- **Contexto**: 32k tokens con compresiÃ³n automÃ¡tica

## ğŸ” Ejemplos de Uso

```bash
# ConversaciÃ³n natural
ğŸ’¬ TÃº: Hola, Â¿puedes ayudarme con Python?
ğŸ¤– Claude: Â¡Por supuesto! Soy experto en Python...

# ExploraciÃ³n
ğŸ’¬ TÃº: /ls src
ğŸ“ Contenido de 'src':
  ğŸ“‚ components/ (5 archivos)
  ğŸ“„ main.py (2.1KB) ğŸ Python

# AnÃ¡lisis
ğŸ’¬ TÃº: /cat main.py
ğŸ“„ Archivo: main.py
ğŸ“Š TamaÃ±o: 2.1KB | LÃ­neas: 67 | Tipo: ğŸ Python
```

## ğŸ› Troubleshooting

### Problema: "No se pudo conectar con Ollama"
**SoluciÃ³n**: Verifica que Ollama estÃ© corriendo:
```bash
ollama list
```

### Problema: "Modelo no disponible"
**SoluciÃ³n**: Descarga el modelo:
```bash
ollama pull deepseek-r1:8b
```

### Problema: Respuestas lentas
**SoluciÃ³n**: 
- Usa el modelo rÃ¡pido para tareas simples
- Aumenta RAM asignada a GPU integrada en BIOS

## ğŸ¤ Contribuciones

Este es un proyecto personal, pero las sugerencias son bienvenidas.

## ğŸ“„ Licencia

MIT License - Usa libremente.

---

**LocalClaude v1.0** - Tu asistente local inteligente ğŸ§ 